{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f8af62-db25-4f11-800a-8685f2bd7a02",
   "metadata": {},
   "source": [
    "#### Cleaning each file and merging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca77e1e5-2308-4c2d-a6fb-27ea266567dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mary\\AppData\\Local\\Temp\\ipykernel_16052\\2465256420.py:7: DtypeWarning: Columns (7,16,17,23,27,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('../data/raw/ntsb_crash_reports.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "ACN                      0\n",
      "Date                     0\n",
      "Local Time Of Day      429\n",
      "Locale Reference       327\n",
      "State Reference        181\n",
      "                     ...  \n",
      "Callback             11562\n",
      "Narrative.1          10767\n",
      "Callback.1           11783\n",
      "Synopsis                 2\n",
      "Unnamed: 125         11812\n",
      "Length: 126, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mary\\AppData\\Local\\Temp\\ipykernel_16052\\2465256420.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
      "C:\\Users\\Mary\\AppData\\Local\\Temp\\ipykernel_16052\\2465256420.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "NtsbNo                      0\n",
      "EventType                  30\n",
      "Mkey                        0\n",
      "EventDate                   7\n",
      "City                       61\n",
      "State                    9299\n",
      "Country                   508\n",
      "ReportNo               176261\n",
      "N                         103\n",
      "HasSafetyRec                0\n",
      "ReportType               7078\n",
      "OriginalPublishDate     20243\n",
      "HighestInjuryLevel     104717\n",
      "FatalInjuryCount            0\n",
      "SeriousInjuryCount          0\n",
      "MinorInjuryCount            0\n",
      "ProbableCause          114958\n",
      "EventID                 91239\n",
      "Latitude                    0\n",
      "Longitude                   0\n",
      "Make                       63\n",
      "Model                     153\n",
      "AirCraftCategory          607\n",
      "AirportID              125163\n",
      "AirportName             63805\n",
      "AmateurBuilt                0\n",
      "NumberOfEngines          6373\n",
      "Scheduled              163590\n",
      "PurposeOfFlight         14055\n",
      "FAR                      6344\n",
      "AirCraftDamage           4366\n",
      "WeatherCondition         5339\n",
      "Operator               126235\n",
      "ReportStatus             6924\n",
      "RepGenFlag                  0\n",
      "DocketUrl              153248\n",
      "DocketPublishDate      153248\n",
      "Unnamed: 37            176620\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mary\\AppData\\Local\\Temp\\ipykernel_16052\\2465256420.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete. Cleaned files saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('../data/raw/asrs_merged_file.csv', skiprows=1)  # Skip the first row with column names\n",
    "df2 = pd.read_csv('../data/raw/ntsb_crash_reports.csv')\n",
    "\n",
    "# Function to clean a DataFrame\n",
    "def clean_dataframe(df):\n",
    "    # 1. Check for missing values\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # 2. Handle missing values (drop or fill)\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
    "\n",
    "    # 3. Data type consistency\n",
    "    # Convert date columns to datetime if applicable\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "    # 4. Data cleansing: Remove whitespace and special characters\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.strip()  # Remove leading/trailing whitespace\n",
    "        df[col] = df[col].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)  # Remove special characters\n",
    "\n",
    "    # 5. Normalize column names (optional)\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]  # Convert to lowercase and replace spaces with underscores\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean both DataFrames\n",
    "cleaned_df1 = clean_dataframe(df1)\n",
    "cleaned_df2 = clean_dataframe(df2)\n",
    "\n",
    "# Save the cleaned DataFrames to new CSV files (optional)\n",
    "cleaned_df1.to_csv('../data/processed/asrs_data_cleaned.csv', index=False)\n",
    "cleaned_df2.to_csv('../data/processed/ntsb_data_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Data cleaning complete. Cleaned files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7b833-cd3e-4d29-bfe9-94a0ab454604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASRS and NTSB datasets...\n",
      "Selecting relevant columns from ASRS and NTSB datasets...\n",
      "Rows in ASRS dataset: 11812\n",
      "Rows in NTSB dataset: 176620\n",
      "Converting date columns and formatting them to remove time components...\n",
      "Creating state abbreviation mapping for merging consistency...\n",
      "Merging the ASRS and NTSB datasets based on date, state, and aircraft make/model...\n",
      "Rows in merged dataset: 188432\n",
      "Rows dropped during merge: 0\n",
      "Saving the merged dataset to a CSV file...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Step 1: Load datasets (with low_memory=False to suppress DtypeWarning)\n",
    "print(\"Loading ASRS and NTSB datasets...\")\n",
    "asrs = pd.read_csv('../data/processed/asrs_data_cleaned.csv')  # Replace with the path to ASRS data\n",
    "ntsb = pd.read_csv('../data/processed/ntsb_data_cleaned.csv', low_memory=False)  # Replace with the path to NTSB data\n",
    "\n",
    "# Step 2: Selecting relevant columns from both datasets\n",
    "print(\"Selecting relevant columns from ASRS and NTSB datasets...\")\n",
    "asrs_cols = [\n",
    "    'date', 'local_time_of_day', 'state_reference', 'latitude_/_longitude_(uas)',\n",
    "    'flight_conditions', 'aircraft_operator', 'make_model_name', 'flight_phase',\n",
    "    'narrative', 'synopsis', 'primary_problem'\n",
    "]\n",
    "ntsb_cols = [\n",
    "    'eventdate', 'state', 'latitude', 'longitude', 'make', 'model',\n",
    "    'highestinjurylevel', 'fatalinjurycount', 'seriousinjurycount',\n",
    "    'minorinjurycount', 'weathercondition', 'aircraftdamage'\n",
    "]\n",
    "\n",
    "# Filter datasets to include only relevant columns\n",
    "asrs_filtered = asrs[asrs_cols]\n",
    "ntsb_filtered = ntsb[ntsb_cols].copy()  # Create a copy to avoid view issues\n",
    "\n",
    "# Step 3: Show number of rows in each dataset\n",
    "print(f\"Rows in ASRS dataset: {len(asrs_filtered)}\")\n",
    "print(f\"Rows in NTSB dataset: {len(ntsb_filtered)}\")\n",
    "\n",
    "# Step 4: Convert date columns and format them to remove time component\n",
    "print(\"Converting date columns and formatting them to remove time components...\")\n",
    "asrs_filtered.loc[:, 'date'] = pd.to_datetime(asrs_filtered['date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "ntsb_filtered.loc[:, 'eventdate'] = pd.to_datetime(ntsb_filtered['eventdate'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# Step 5: Create state abbreviation mapping for merging consistency\n",
    "print(\"Creating state abbreviation mapping for merging consistency...\")\n",
    "state_mapping = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',\n",
    "    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',\n",
    "    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "# Map NTSB states to abbreviations\n",
    "ntsb_filtered.loc[:, 'state_abbr'] = ntsb_filtered['state'].map(state_mapping)\n",
    "\n",
    "# Step 6: Merge datasets on date, state, and aircraft make/model\n",
    "print(\"Merging the ASRS and NTSB datasets based on date, state, and aircraft make/model...\")\n",
    "merged = pd.merge(\n",
    "    asrs_filtered,\n",
    "    ntsb_filtered,\n",
    "    left_on=['date', 'state_reference', 'make_model_name'],\n",
    "    right_on=['eventdate', 'state_abbr', 'make'],\n",
    "    how='outer',  # Perform an outer join to include all records from both datasets\n",
    "    suffixes=('_asrs', '_ntsb')\n",
    ")\n",
    "\n",
    "# Step 7: Show number of rows after merge\n",
    "print(f\"Rows in merged dataset: {len(merged)}\")\n",
    "\n",
    "# Step 8: Calculate how many rows were dropped during the merge (if any)\n",
    "rows_dropped = len(asrs_filtered) + len(ntsb_filtered) - len(merged)\n",
    "print(f\"Rows dropped during merge: {rows_dropped}\")\n",
    "\n",
    "# Step 9: Save the merged dataset to a CSV file\n",
    "print(\"Saving the merged dataset to a CSV file...\")\n",
    "merged.to_csv('../data/processed/merged_asrs_ntsb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
